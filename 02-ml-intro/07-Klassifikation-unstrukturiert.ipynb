{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pb_PBThH0Pri"
   },
   "source": [
    "# Klassifikation mit unstrukturierten Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5q2F00O0Pry"
   },
   "source": [
    "## Unstrukturierte Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir beschäftigen und jetzt mit Textdaten. Dazu haben wir alle Toplevel-Posts des [Technology Subreddit](https://www.reddit.com/r/Technology) heruntergeladen und in einer CSV-Datei gespeichert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRvS1KLp0Pry"
   },
   "source": [
    "Bei unstrukturierten Daten musst du erst Features finden. Bei Bildern ist das z.B. sehr schwierig, deswegen wird dort fast immer nur Deep Learning verwendet. Dort macht der erste Layer aus den Pixel eine Art Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    os.system(\"test -f reddit-technology-toplevel-title.csv.xz || wget  https://github.com.com/heiseacademy/ml-python/blob/main/02-ml-intro/reddit-technology-toplevel-title.csv.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HuPGQO20Pry"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "docs = pd.read_csv(\"reddit-technology-toplevel-title.csv.xz\", parse_dates=[\"created_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpRii6_O0Pry"
   },
   "source": [
    "Wie du siehst, sind das *viele* Posts, nämlich fast zwei Millionen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuD25gL50Prz"
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDl7E1QY0Prz"
   },
   "source": [
    "Sind die Daten hinreichend aktuell? Solche Informationen überprüfst du besser gleich ganz zu Beginn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Utg0unJl0Pr0"
   },
   "outputs": [],
   "source": [
    "docs.set_index(\"created_utc\").resample(\"Q\").count().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v8Ajx4l0Pr0"
   },
   "source": [
    "Das ist zwar nicht perfekt, aber es eignet sich doch hinreichend gut für eine Analyse.\n",
    "\n",
    "Um die Daten zu vektorisieren, solltest du zuerst die Stopwords eliminieren. Das ist normalerweise ein iterativer Prozess, hier findest du die etwas ergänzte Stopword-Liste von `spacy` (damit du das nicht noch installieren musst):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pq5J3J2e0Pr0"
   },
   "outputs": [],
   "source": [
    "stop_words= {\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', \n",
    "             'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', \n",
    "             'amongst', 'amount', 'amp', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', \n",
    "             'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', \n",
    "             'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', \n",
    "             'beyond', 'blog', 'body', 'both', 'bottom', 'but', 'buy', 'buycheap', 'by', 'ca', 'call', 'can', 'cannot', \n",
    "             'case', 'change', 'co', 'com', 'could', 'create', 'delete', 'did', 'do', 'does', 'doing', 'done', 'down', \n",
    "             'download', 'drive', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'email', \n",
    "             'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', \n",
    "             'fifteen', 'fifty', 'first', 'five', 'fix', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', \n",
    "             'full', 'further', 'get', 'give', 'go', 'good', 'had', 'has', 'have', 'he', 'help', 'hence', 'her', 'here', \n",
    "             'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', \n",
    "             'http', 'https', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', \n",
    "             'last', 'late', 'latter', 'latterly', 'least', 'less', 'll', 'look', 'made', 'make', 'many', 'market', 'may', \n",
    "             'me', 'meanwhile', 'message', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', \n",
    "             'must', 'my', 'myself', \"n't\", 'name', 'namely', 'need', 'neither', 'never', 'nevertheless', 'new', 'news', \n",
    "             'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'number', 'n‘t', \n",
    "             'n’t', 'of', 'off', 'often', 'on', 'once', 'one', 'online', 'only', 'onto', 'or', 'other', 'others', \n",
    "             'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'page', 'part', 'pass', 'per', 'perhaps', \n",
    "             'please', 'post', 'put', 'question', 'quite', 'rather', 're', 'really', 'reddit', 'regarding', 'remove', \n",
    "             'review', 'same', 'say', 'search', 'see', 'seem', 'seemed', 'seeming', 'seems', 'self', 'send', 'serious', \n",
    "             'several', 'she', 'should', 'show', 'side', 'since', 'site', 'six', 'sixty', 'so', 'some', 'somehow', \n",
    "             'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'support', 'take', 'ten', \n",
    "             'test', 'text', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', \n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', \n",
    "             'though', 'three', 'through', 'throughout', 'thru', 'thus', 'time', 'to', 'together', 'too', 'top', \n",
    "             'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'unless', 'unlock', 'until', 'up', 'upon', \n",
    "             'us', 'use', 'used', 'using', 'various', 've', 'very', 'via', 'video', 'was', 'watch', 'way', 'we', 'well', \n",
    "             'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', \n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', \n",
    "             'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'would', 'yet', 'you', 'your', \n",
    "             'yours', 'yourself', 'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', \n",
    "             '’s', '’ve'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXTX4GFF0Pr2"
   },
   "source": [
    "Für die Vektorisierung von Dokumente werden normalerweise die Wörter gezählt und als Features verwendet. Damit besonders häufige Wörter nicht zu stark dominieren, werden die mit der sog. \"Inverierten Dokumentenfrequenz\" abgewertet.\n",
    "\n",
    "`scikit-learn` kann das mit dem `TfidfVectorizer` alles für dich erledigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MMl7E-q0Pr2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, stop_words=list(stop_words))\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(docs[\"title\"].map(str))\n",
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtdscgY-0Pr3"
   },
   "source": [
    "Wie du siehst, ist die entstehende Matrix mit fast zwei Millionen Zeilen und knap 45.000 Spalten *sehr groß*. Da die meisten Wörter in den meisten Dokumenten nicht vorkommen, besteht sie zu einem ganz großen Teil aus leeren Einträgen und kann daher als sog. *sparse matrix* sehr effizient abgelegt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SB2SgOeu0Pr3"
   },
   "outputs": [],
   "source": [
    "tfidf_vectors.data.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FwnKomg0Pr4"
   },
   "source": [
    "Bei einer `float`-Darstellung mit vier Bytes pro `float` würde das in einer naiven Darstellung 1922041 x 44978 x 4 Bytes = 322 GB RAM benötigen. Zum Glück sind es hier nur 130 MB - ein Effizienzgewinn fast um den Faktor 3.000. Nur deswegen funktioniert das überhaupt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSvBRglG0Pr_"
   },
   "source": [
    "### Reddit-Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VNQmV9I0Pr_"
   },
   "source": [
    "Betrachten wir unser echtes Datenset, gibt es auch hier Kategorien, nämlich die Flairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LswcNL2V0Pr_"
   },
   "outputs": [],
   "source": [
    "docs.groupby(\"flair\").agg({\"title\": \"count\"}).sort_values(\"title\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cyOsakp0PsA"
   },
   "source": [
    "Passend zum Videokurs suchen wir uns den Flair *Artificial Intelligence* aus und setzen dessen *Target* auf 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RGpzry60PsA"
   },
   "outputs": [],
   "source": [
    "docs[\"target\"] = 0\n",
    "docs.loc[docs[\"flair\"] == \"Artificial Intelligence\", \"target\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_tJYEdW0PsA"
   },
   "source": [
    "Nun sind allerdings die Dokumente ohne diesen Flair deutlich überrepräsentiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U20wbql90PsA"
   },
   "outputs": [],
   "source": [
    "ai = docs[docs[\"flair\"] == \"Artificial Intelligence\"]\n",
    "len(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vL8vFefZ0PsA"
   },
   "outputs": [],
   "source": [
    "non_ai = docs[docs[\"flair\"] != \"Artificial Intelligence\"]\n",
    "len(non_ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmCghZ0e0PsB"
   },
   "source": [
    "Damit kann ein Modell nicht gut trainiert werden, weil viel zu viele negative Beispiele den Klassifikator in eine falsche Richtung drängen. Deswegen *stratifizieren* wir das Datenset und nehmen so viele negative wie positive Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwV5FOAW0PsB"
   },
   "outputs": [],
   "source": [
    "data = pd.concat([ai, non_ai.sample(n = len(ai), random_state=42)])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjJc8HNv0PsB"
   },
   "source": [
    "Du kannst weiter mit dem bereits trainierten Vectorizer arbeiten und rufst daher nur dessen `transform`-Methode auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nb4t84yW0PsB"
   },
   "outputs": [],
   "source": [
    "ai_tfidf_vectors = tfidf_vectorizer.transform(data[\"title\"].map(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFwR34_h0PsB"
   },
   "source": [
    "Um die Ergebnisqualität messen zu können, teilst du das Datenset auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0h587Aje0PsB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(ai_tfidf_vectors, data['target'], test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exMMV5-e0PsC"
   },
   "source": [
    "Der Klassifikator ist schnell trainiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "porrHSvj0PsC"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "ai_svm = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
    "ai_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgczeU-o0PsC"
   },
   "source": [
    "Das Ergebnis ist schon ziemlich gut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-zVqpoQ0PsC"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, ai_svm.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp_kUlpB0PsD"
   },
   "source": [
    "Mit 87% Genauigkeit (hier passt die Accuracy wegen dem gleichverteilten Datenset) kannst du jetzt vorhersagen, ob ein Post von AI handelt oder nicht."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML Python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
