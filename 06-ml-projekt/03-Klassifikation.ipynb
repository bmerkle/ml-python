{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du hast im letzten Teil schon gesehen, wie sich Daten von Reddit herunterladen lassen. Auch die Flairs hast du kennengelernt.\n",
    "\n",
    "Allerdings hast du auch gesehen, dass nicht für alle Posts die Flairs richtig bereitgestellt werden:\n",
    "* Es gibt alte Posts, bei denen die Flairs noch nicht eingeführt waren\n",
    "* Nicht alle Autoren verwenden die Flairs konsistent\n",
    "\n",
    "Mithilfe der Klassifikation hast du aber die Möglichkeite, auch die bisher nicht kategorisierten Daten noch den Flairs richtig zuzuordnen. Das wirst du in diesem Notebook durchführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achtung\n",
    "\n",
    "Dieser Teil des Notebooks benötigt das gesamte Technology-Subreddit, das für den Download leider zu groß ist (> 10 GB). Daher ist dies im Colab-Notebook nicht ablauffähig! Eine Erklärung, wie du diese Daten selbst akquirieren kannst, findest du z.B. im iX-Artikel [Beziehungssache](https://www.heise.de/select/ix/2021/7/2102513144636338770). \n",
    "\n",
    "Du benötigst dies aber nur für die Erzeugung einer kleineren Datenmenge, die du dann anschließend im Notebook ausschließlich verwenden wirst und selbstverständlich dazu herunterladen kannst.\n",
    "\n",
    "Bitte ab hier deshalb vorerst nur zuschauen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = sqlite3.connect(\"technology.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst lädst du alle Posts ein. Viele Felder benötigst du nicht, um Speicher zu sparen, benutzt du nur die notwendigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_sql(\"SELECT id, url, created_utc, title, flair, parent_id, author, score, selftext, body FROM posts p\", \n",
    "                  sql, parse_dates=[\"created_utc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flairs wurden erst 2015 eingeführt und gelten nur für Toplevel-Posts. Du möchtest nur Posts verwenden, bei denen ein Flair gesetzt wurde, sonst könnten die Autoren das evtl. vergessen haben. Da du den `DataFrame` später noch modifizieren wirst, erzeugst du eine Kopie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2015 = posts[(posts[\"created_utc\"].dt.year>=2015) & \n",
    "                posts[\"parent_id\"].isna() & \n",
    "                ~posts[\"flair\"].isna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du bestimmst nun, ob die Posts zu Transportation gehören oder nicht. Beachte dabei, dass die Flairs *Transport* und *Transportation* genannt wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2015[\"target\"] = top2015[\"flair\"].isin([\"Transport\", \"Transportation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie verteilen sich die positiven und negativen Beispiele?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2015.value_counts(\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daraus erzeugst du ein stratifiziertes Datenset mit gleich vielen positiven wie negativen Beispielen: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = top2015[top2015[\"target\"] == True]\n",
    "neg = top2015[top2015[\"target\"] == False]\n",
    "data = pd.concat([pos, neg.sample(n = len(pos), random_state=42)], \n",
    "                 ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werden die Daten vektorisiert. Du verzichtest dabei auf Stopwords etc., die üblicherweise in der Textanalyse verwendet werden und vertraust auf die Funktionsweise von TF/IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1), max_df=0.7, min_df=5)\n",
    "tfidf_vectors = tfidf.fit_transform(data[\"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Konvention nennst du die unabhängige Variable `X` und die abhängige `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_vectors\n",
    "y = data[\"target\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out-Verfahren: Getrennte Mengen für Training und Test\n",
    "\n",
    "Du teilst die Datenmenge in einen Teil, mit dem du den Klassifikator trainierst (75%) und eine, mit dem du die Ergebnisse verifizierst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T20:08:34.239506Z",
     "start_time": "2018-09-18T20:08:34.227489Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Klassifikationsmodell wird als Support Vector Machine nur mit Trainingsdaten trainiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du führst eine Vorhersage für die (dem Klassifikator unbekannten) Testdaten durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und betrachtest die Ergebnisse der Klassifikation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% bzw. 96% Precision und Recall sind ziemlich gut, mit diesem Modell kannst du arbeiten und nun alle Posts (größtenteils richtig) klassifizieren:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesamtes Datenset klassifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun möchtest du die Trainingsdaten nutzen, um das gesamte oben eingelesene Datenset zu klassifizieren. Auch in den Kommentaren können nützliche Informationen enthalten sein, deswegen nutzt du die auch.\n",
    "\n",
    "Dazu musst du zunächst den Text für alle Posts bestimmen. Dazu fügst du die einzelnen Textkomponenten mit Leerzeichen zusammen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"text\"] = posts[\"title\"].map(str) + \" \" + posts[\"body\"].map(str) + \" \" + posts[\"selftext\"].map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wendest du den Klassifikator an und klassifizierst alle Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[\"transport\"] = clf.predict(tfidf.transform(posts[\"text\"].map(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du interessierst dich für die Posts, die der Klassifikator als zu \"Transport\" zugehörig erkennt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport = posts[posts[\"transport\"] == True].copy()\n",
    "transport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten speicherst du nun in einer CSV-Datei ab. Vorher ersetzt du noch die Umbrüche durch Leerzeichen, damit das CSV-File auch wieder richtig eingelesen werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport[\"text\"] = transport[\"text\"].str.replace(\"\\n\", \" \")\n",
    "transport[\"text\"] = transport[\"text\"].str.replace(\"\\r\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.set_index(\"id\")[[\"created_utc\", \"url\", \"parent_id\", \"author\", \"score\", \"text\"]].\\\n",
    "          to_csv(\"transport-all-comments.csv\", index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zwischenergebnis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach all der Vorarbeit hast du  nun die Daten selektiert, mit denen du jetzt ausschließlich arbeiten wirst. \n",
    "\n",
    "Der Vorbereitungsaufwand mag hoch erscheinen, allerdings hast du nun auch wirklich ein Datenset, was genau zu deinen Geschäftsanforderungen passt. Mit manuellen Methoden wäre das nicht mit vertretbarem Aufwand möglich gewesen! \n",
    "\n",
    "Das ist auch ein Grund für die Beliebtheit von Data Science: mit relativ moderatem Aufwand können genau passende Datenmenge erzeugt werden!\n",
    "\n",
    "Die Transport-Daten sind in einer SQLite-Datenbank abgespeichert, mit der du ab jetzt weiterarbeiten wirst. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
