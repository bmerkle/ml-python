{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25a90b9",
   "metadata": {},
   "source": [
    "# Performance-Metriken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e244e1",
   "metadata": {},
   "source": [
    "In den letzten Teilen hast du Dimensionsreduktion, Clustering und Topic-Modelle kennengelernt. All diese unüberwachten Methoden haben Parameter, die du optimieren kannst. Doch welche Variante funktioniert am besten? Um das beurteilen zu können, benötigst du *Metriken*, die du optimieren kannst.\n",
    "\n",
    "Im Fall der Dimensionsreduktion ist das (scheinbar) ziemlich einfach. Du kannst die Transformation einfach wieder rückwärts durchführen und die Abweichungen bestimmen. Dieser sog. *quadratische Fehler* wird häufig dazu verwendet. Allerdings ist das nicht immer sinnvoll, denn ab und zu wirst du andere Kriterien benötigen, z.B. das \"ähnliche\" Vektoren möglichst zusammengefasst werden. t-SNE kann keine Rücktransformationen durchführen - bei UMAP geht das eingeschränkt.\n",
    "\n",
    "Etwas einfacher sieht es bei Clustering aus, da gibt es das sog. *Silhouette-Maß*. Das kannst du besonders auf *konvexe Cluster* anwenden. Außerdem gibt es noch den *Calinski-Harabasz-Score*.\n",
    "\n",
    "Bei Topic-Modellen kannst du die *Perplexität* benutzen. Das Maß drückt aus, wie gut ein bisher unbekanntes Dokument sich in die Topic-Struktur eingliedert. Leider entspricht das Maß nicht der menschlichen Intuition, deswegen wirst du dich hier auch auf die *Kohärenz* der Topics konzentrieren. Die misst, wie gut die Wörter eines Topics zusammenpassen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a1e5d",
   "metadata": {},
   "source": [
    "## Dimensionsreduktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e204b97",
   "metadata": {},
   "source": [
    "Unsere Experimente nutzen wie gehabt das Digit-Datenset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06208c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068aab8f",
   "metadata": {},
   "source": [
    "Du kannst nun eine PCA in zwei Dimensionen durchführen und den mittleren quadratischen Fehler selbst bestimmen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "data2 = pca2.fit_transform(digits[\"data\"])\n",
    "data_inverse = pca2.inverse_transform(data2)\n",
    "np.square(digits[\"data\"] - data_inverse).sum() / len(digits[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac664024",
   "metadata": {},
   "source": [
    "Sicher fällt der Fehler mit zunehmender Anzahl an Dimensionen. Das kannst du einfach verifizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "for n in range(2, 65):\n",
    "    pca_n = PCA(n_components=n)\n",
    "    data_n = pca_n.fit_transform(digits[\"data\"])\n",
    "    data_inverse_n = pca_n.inverse_transform(data_n)\n",
    "    err.append((n, np.square(digits[\"data\"] - data_inverse_n).sum() / len(digits[\"data\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea03849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(err, columns=[\"n\", \"err\"]).set_index(\"n\").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735f5ee",
   "metadata": {},
   "source": [
    "Bei t-SNE kannst du die *Kullback-Leibler-Divergenz* als Maß verwenden. Sie zeigt die, wie stark sich die Verteilungen in dem ursprünglichen und transformierten Raum unterscheiden. Je näher sie an `0` liegt, desto besser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb2895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne2 = TSNE(n_components=2, random_state=42)\n",
    "tdata2 = tsne2.fit_transform(digits[\"data\"])\n",
    "tsne2.kl_divergence_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79739be",
   "metadata": {},
   "source": [
    "UMAP unterstützt wiederum die Rücktransformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843832c0-65b4-407d-a2a9-9109b9ea0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install umap-learn[parametric_umap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed382d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "umap2 = umap.UMAP(n_components=2, random_state=42)\n",
    "udata2 = umap2.fit_transform(digits[\"data\"])\n",
    "udata_inverse = umap2.inverse_transform(udata2)\n",
    "np.square(digits[\"data\"] - udata_inverse).sum() / len(digits[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e42cbd",
   "metadata": {},
   "source": [
    "Wie erwartet ist das Ergebnis besser als das von PCA in zwei Dimensionen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3171b28",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398db71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km5 = KMeans(n_clusters=5, random_state=42).fit(digits[\"data\"])\n",
    "km10 = KMeans(n_clusters=10, random_state=42).fit(digits[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004cdca",
   "metadata": {},
   "source": [
    "Der Silhouette-Score ist idealerweise `1`, `0` bedeutet Überlappungen.\n",
    "Calinski-Harabasz ist größer, je besser die Cluster separiert sind.\n",
    "\n",
    "Probiere die Scores mit zwei unterschiedlichen Cluster-Konfigurationen aus, die jeweils mit K-Means berechnet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d3840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "print(silhouette_score(digits[\"data\"], km5.labels_))\n",
    "print(silhouette_score(digits[\"data\"], km10.labels_))\n",
    "print(calinski_harabasz_score(digits[\"data\"], km5.labels_))\n",
    "print(calinski_harabasz_score(digits[\"data\"], km10.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c6060d",
   "metadata": {},
   "source": [
    "Die Birch-Cluster waren zuletzt besser, wird das auch in den Metriken abgebildet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "birch5 = Birch(n_clusters=5).fit(digits[\"data\"])\n",
    "birch10 = Birch(n_clusters=10).fit(digits[\"data\"])\n",
    "\n",
    "print(silhouette_score(digits[\"data\"], birch5.labels_))\n",
    "print(silhouette_score(digits[\"data\"], birch10.labels_))\n",
    "print(calinski_harabasz_score(digits[\"data\"], birch5.labels_))\n",
    "print(calinski_harabasz_score(digits[\"data\"], birch10.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e8ef4",
   "metadata": {},
   "source": [
    "Der Score sieht tatsächlich bei Birch schlechter aus, auch wenn die Ergebnisse eher den echten Zahlen entsprechen. Das liegt möglicherweise daran, dass die echten Ergebnisse gar nicht konvex im Raum verteilt sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c9759",
   "metadata": {},
   "source": [
    "## Topic-Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e1131",
   "metadata": {},
   "source": [
    "Hier berechnest du wieder die gewohnten Topic-Modelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e532054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "news = datasets.fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "STOPWORDS = {'none', 'thereby', 'mine', 'serious', 'whereafter', 'nothing', \"'ll\", \n",
    "             'itself', 'first', 'whoever', '’ve', 'noone', 'moreover', 'regarding', \n",
    "             'but', 'various', 'and', 'their', 'between', 'everyone', 'us', 'other', \n",
    "             'third', 'last', 'only', 'been', 'always', 'throughout', 'over', 'anyhow', \n",
    "             'i', 'nobody', 'be', 'off', \"'d\", 'then', 'eleven', 'since', \"'ve\", 'did', \n",
    "             'ever', 'than', 'call', 'few', 'could', 'whatever', 'front', 'there', \n",
    "             'across', 'whenever', 'is', 'this', 'empty', 'indeed', 'please', 'namely', \n",
    "             'his', 'eight', 'those', 'hence', 'wherein', 'amongst', 'using', 'both', \n",
    "             '’re', 'seem', 'two', 'several', 'whether', 'about', 'due', 'behind', 'am', \n",
    "             'what', 'name', 'has', 'three', 'therefore', '‘s', 'whereas', 'the', 'until', \n",
    "             'meanwhile', 'anything', 'that', 'never', 'how', 'sometimes', 'each', \n",
    "             'toward', 'doing', 'someone', 'at', 'hereafter', 'almost', 'if', 'same', \n",
    "             'her', 'anyone', 'became', 'into', 'latter', 'by', \"'s\", 'four', 'wherever', \n",
    "             'besides', 'must', 'thence', 'in', 'anywhere', 'any', 'twelve', 'out', 'it', \n",
    "             'one', 'least', 'used', '‘ll', 'put', 'therein', 'a', 're', 'she', 'are', \n",
    "             'beforehand', 'my', 'through', 'ten', 'go', 'too', '’m', 'either', 'below', \n",
    "             'else', 'around', 'all', 'except', 'n‘t', 'not', 'such', '‘re', 'was', '’s', \n",
    "             'may', 'whence', 'also', 'another', 'beyond', 'without', 'perhaps', 'alone', \n",
    "             'should', 'nevertheless', 'own', 'he', 'these', 'seemed', 'give', 'made', \n",
    "             'some', 'part', 'on', 'himself', 'hereupon', 'whereupon', 'six', 'via', 'of', \n",
    "             'quite', \"'m\", 'however', 'onto', 'as', 'sometime', 'more', 'while', 'sixty', \n",
    "             'does', 'everywhere', 'elsewhere', 'whither', 'who', 'nor', 'seeming', \n",
    "             'formerly', 'nowhere', 'our', 'former', 'hereby', 'further', \"'re\", \n",
    "             'can', 'thus', 'something', 'why', 'themselves', 'were', 'amount', 'do', \n",
    "             'we', 'beside', 'mostly', 'they', 'very', 'your', 'somewhere', 'upon', 'so', \n",
    "             'them', 'latterly', 'neither', 'within', 'enough', 'hers', 'cannot', 'you', \n",
    "             'every', 'most', 'ca', 'show', 'will', 'being', 'after', 'though', 'fifteen', \n",
    "             'down', 'really', 'although', 'full', 'up', 'well', 'somehow', 'yourself', 'me', \n",
    "             'bottom', 'next', 'many', 'unless', 'or', 'anyway', 'five', 'for', 'say', \n",
    "             'twenty', 'would', 'otherwise', 'nine', 'no', 'against', 'ourselves', 'just', \n",
    "             'even', 'yet', 'above', '‘d', 'again', 'already', 'others', 'before', 'forty', \n",
    "             'here', 'move', '‘m', \"n't\", 'with', 'now', 'seems', 'n’t', 'among', 'which', \n",
    "             'towards', 'side', 'still', 'might', 'together', '’ll', 'from', 'everything', \n",
    "             'have', 'becoming', 'keep', 'become', 'often', 'herein', 'under', 'whereby', \n",
    "             'top', 'thru', 'becomes', 'where', 'along', 'during', 'whole', 'him', 'once', \n",
    "             'to', 'afterwards', 'back', 'its', 'get', 'rather', 'because', 'hundred', \n",
    "             'make', 'see', 'thereafter', 'done', 'thereupon', 'had', '‘ve', 'ours', \n",
    "             'yours', 'much', 'an', 'per', 'whose', 'fifty', 'myself', 'take', 'less', \n",
    "             'whom', 'yourselves', 'when', 'herself', '’d',\n",
    "             'edu', 'university', 'article', 'writes', 'posting', 'nntp', 'host', \n",
    "             'organization', 'subject', 'state', 'com', 'netcom', 'uk', 'ac', 'cs', \n",
    "             'caltech', 'gov', 'jpl' }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adebdb",
   "metadata": {},
   "source": [
    "Je nach Topic-Modell musst du unterschiedliche vektorisieren. Für das erste Verfahren, die nicht-negative Matrixfaktorisierung, solltest du TF/IDF verwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672d33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = list(STOPWORDS), min_df=5)\n",
    "vec = tfidf.fit_transform(news[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b0e9cb",
   "metadata": {},
   "source": [
    "Für die Berechnung der Kohärenz benötigst du entweder `gensim`, ein komplett anderes Paket für Topic-Modelle oder `tmtoolkit`, das diese Größen auch für die Modelle von `scikit-learn` berechnen kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f42bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tmtoolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd49226",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d76004",
   "metadata": {},
   "source": [
    "Du importierst die Methode zur Berechnung der *Coherence-Scores*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb047113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8846a9",
   "metadata": {},
   "source": [
    "Die Ergebnisse der Modelle möchtest du dir auch wieder anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e77232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    # wie oben, nur als Text\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        first_index = topic.argsort()[-1]\n",
    "        print(\"Topic %s (%02d):\" % (feature_names[first_index], topic_idx))\n",
    "        print(\" \".join([\"'\"+feature_names[i]+\"'\"\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b00fb3",
   "metadata": {},
   "source": [
    "Du beginnst mit dem normalen Modell mit 10 Topics und lässt es nochmal anzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f5c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=10, random_state=42)\n",
    "nmf.fit(vec)\n",
    "display_topics(nmf, tfidf.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92297b9",
   "metadata": {},
   "source": [
    "Jetzt berechnest du den Coherence-Score für die einzelnen Topics. `c_v` ist eine Art der Berechnung, die Werte sollen sich dabei möglichst nah an `1` bewegen. Lass dir zusätzlich den Mittelwert ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e06b7-a8ec-4809-93be-5a228191479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ab218",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=nmf.components_, \n",
    "                        dtm=vec, \n",
    "                        vocab=np.array(tfidf.get_feature_names_out()), \n",
    "                        texts=tfidf.inverse_transform(vec))\n",
    "res, sum(res)/len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f335355",
   "metadata": {},
   "source": [
    "In einer Schleife kannst du die Coherence-Werte für unterschiedliche Anzahlen von Topics berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f008a34-79dc-4afd-9d73-3d595296a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827ca99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "texts = tfidf.inverse_transform(vec)\n",
    "coh = []\n",
    "for n in trange(5, 21):\n",
    "    nmf = NMF(n_components=n, random_state=42)\n",
    "    nmf.fit(vec)\n",
    "    res = metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=nmf.components_, \n",
    "                        dtm=vec, \n",
    "                        vocab=np.array(tfidf.get_feature_names_out()), \n",
    "                        texts=texts)\n",
    "    coh.append((n, sum(res)/len(res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3183ebb",
   "metadata": {},
   "source": [
    "Visualisiere das Ergebnis und suche den optimale Wert heraus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6cb98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(coh, columns=[\"n\", \"coherence\"]).set_index(\"n\").plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d241a59",
   "metadata": {},
   "source": [
    "11 Topics scheinen eine bessere Kohärenz zu haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793502d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=11, random_state=42)\n",
    "nmf.fit(vec)\n",
    "display_topics(nmf, tfidf.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f19e40",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9fb52",
   "metadata": {},
   "source": [
    "Das gleiche Verfahren führst du nun für LDA durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc696244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1809d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(stop_words = list(STOPWORDS), use_idf=False)\n",
    "cvec = cv.fit_transform(news[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2025ff",
   "metadata": {},
   "source": [
    "Der Aufruf ist identisch zu NMF, allerdings ist die Rechenzeit deutlich länger.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = cv.inverse_transform(cvec)\n",
    "tcoh = []\n",
    "for n in trange(5, 20):\n",
    "    lda = LatentDirichletAllocation(n_components=n, random_state=42)\n",
    "    lda.fit(cvec)\n",
    "    res = metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda.components_, \n",
    "                        dtm=cvec, \n",
    "                        vocab=np.array(cv.get_feature_names_out()), \n",
    "                        texts=texts)\n",
    "    tcoh.append((n, sum(res)/len(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(coh, columns=[\"n\", \"coherence\"]).set_index(\"n\").plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886307cb",
   "metadata": {},
   "source": [
    "Die Scores sind insgesamt niedriger, was der Wahrnehmung entspricht. Die LDA-Topics sind schlechter zu interpretieren. Schau dir auch hier das optimale Ergebnis an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a975f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=11, random_state=42)\n",
    "lda.fit(cvec)\n",
    "display_topics(lda, cv.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb46660",
   "metadata": {},
   "source": [
    "## Metriken auf jeden Fall mit berücksichtigen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b124d5",
   "metadata": {},
   "source": [
    "Auch wenn die Intuition oder Visualisierung dir oft hilft, solltest du unbedingt wenn möglich immer Metriken berechnen, um dich von der Validität deiner Ergebnisse zu überzeugen.\n",
    "\n",
    "Für Dimensionsreduktion, Clustering und Topic-Modelle gibt es solche Metriken. Beim überwachten Lernen werden Metriken eine fast noch wichtigere Rolle spielen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
