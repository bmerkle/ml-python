{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e19f0ca",
   "metadata": {},
   "source": [
    "# Datenvorbereitung: Vektorisierung unstrukturierter Daten und Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bb8f2",
   "metadata": {},
   "source": [
    "Bisher hast du dich mit strukturierten Daten beschäftigt, wo du häufig die (numerischen) Werte direkt als Features nutzen konntest. In diesem Teil wirst du unstrukturierte Daten betrachten und dich dabei speziell auf Texte konzentrieren. Hier liegt der Fall nicht ganz so einfach, weil du zunächst Features finden musst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b458c7",
   "metadata": {},
   "source": [
    "## Texte vektorisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc4fe51",
   "metadata": {},
   "source": [
    "Texte sind ein Spezialfall, weil sie sich relativ einfach in Vektorform überführen lassen und es ein Standardverfahren dafür gibt, das sich für einfache Textanalyse sehr gut bewährt hat.\n",
    "\n",
    "Willst du mehrere Dokumente *vektorisieren*, stellst du dir dazu zunächst das Vokabular auf und nummerierst es durch. Deine Vektoren sind also so groß wie das Vokabular, das kann ziemlich viel Wörter umfassen. \n",
    "\n",
    "Für jedes Dokument berechnest du dir anschließend, wie häufig die Wörter darin vorkommen und setzt diese Anzahl an die entsprechende Vektorposition. Ein solcher Dokumentvektor besteht zu erheblichen Teilen aus `0`, er ist also dünn besetzt. Das gleiche gilt für die Menge aller Dokumentvektoren, die sog. *Dokumenten-Term-Matrix*.\n",
    "\n",
    "`scikit-learn` unterstützt dich dabei sehr gut. Zunächst kannst du mit einem eingebauten Datenset arbeiten, das Posts aus 20 Newsgroups (aus der Anfang des Internets bzw. Usenets) enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac601532",
   "metadata": {},
   "source": [
    "Die Textdaten findest du im Feld `data` des zurückgelieferten `dict`, die jeweilige Newsgroup in `target`. Am besten konvertierst du das in einen `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "newsgroups_df = pd.DataFrame(newsgroups.data, columns=[\"text\"])\n",
    "newsgroups_df[\"newsgroup\"] = [newsgroups.target_names[t] for t in newsgroups.target]\n",
    "newsgroups_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23624a1c",
   "metadata": {},
   "source": [
    "Und jetzt kannst du fast schon mit der Vektorisierung Learning beginnen. Dabei werden die Texte in einzelne Wörter unterteilt (tokenisiert) und anschließend die Wörter durchnummeriert. Für jedes Dokument wird gezählt, welches Wort wie häufig darin vorkommt. So entstehen die Dokumentvektoren:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14e52e",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57787c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cvectors = cv.fit_transform(newsgroups_df[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72350b9b",
   "metadata": {},
   "source": [
    "Nun kannst du dir anschauen, wie das entstehende Objekt aussieht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff8f3f3",
   "metadata": {},
   "source": [
    "Hier kannst du erkennen, dass es sich um eine *sehr große Matrix* handelt. In den 11.314 Posts sind über 130.000 unterschiedliche Wörter aufgetaucht. Zum Glück kommen in den meisten Dokumente nur sehr wenige Wörter vor, so dass die Matrix [dünn besetzt](https://de.wikipedia.org/wiki/D%C3%BCnnbesetzte_Matrix) ist. Diese sog. Sparse Matrices können sehr effizient dargestellt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectors.data.nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42ffa5",
   "metadata": {},
   "source": [
    "Die gesamte Matrix braucht nicht mal 1,5 MB RAM. In einer naiven Darstellung wären das hingegen `11314 * 130107 * 4 Bytes = 5888122392 Bytes`, was fast 6 GB entspricht. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33467455",
   "metadata": {},
   "source": [
    "Eine nette Anwendung des `CountVectorizers` ist die Bestimmung der häufigsten Wörter im gesamten Datenset. Dazu summierst du die Werte einfach auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db74be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = cvectors.sum(axis=0).A[0]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ca119",
   "metadata": {},
   "source": [
    "Die Zahlen entsprechen dabei den Anzahlen der Wörter im Vokabular. Die häufigsten 10 kannst du so ermitteln (das `[::-1]` dient zur Umkehrung der Sortierreihenfolge, so dass die häufigsten am Anfang stehen): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = freq.argsort()[::-1][0:10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ffb34",
   "metadata": {},
   "source": [
    "Nun kannst du das Vokabular nutzen, um dir die Wörter ausgeben zu lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = cv.get_feature_names_out()\n",
    "{ voc[pos]: freq[pos] for pos in top10 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0632c13",
   "metadata": {},
   "source": [
    "Wie du siehst, haben die häufigsten Wörter fast keine Bedeutung und werden normalerweise als sog. *Stoppwörter* weggelassen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594d9f0",
   "metadata": {},
   "source": [
    "### Feature-Engineering mit `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f87a8",
   "metadata": {},
   "source": [
    "Dennoch können solche umfangreichen Vektoren unhandlich sein. Du kannst die Dimension reduzieren, indem du z.B. nur Wörter berücksichtigst, die in mindestens 5 Dokumenten vorkommen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=5)\n",
    "cvectors = cv.fit_transform(newsgroups_df[\"text\"])\n",
    "cvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183963f",
   "metadata": {},
   "source": [
    "Der `CountVectorizer` hat viele Optionen, mit denen die Dimension beeinflusst werden kann. So kannst du etwa die Dimension explizit vorgeben, aber auch die Maximalanzahl der Dokumente vorgeben, in denen Wörter vorkommen dürfen.\n",
    "\n",
    "Schließlich kannst du auch sog. [N-Gramme](https://de.wikipedia.org/wiki/N-Gramm) nutzen, mit denen du Wortkombinationen berücksichtigst. Achtung, dadurch wächst die Dimension beträchtlich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvn = CountVectorizer(ngram_range=(1,2), min_df=5)\n",
    "cvectorsn = cvn.fit_transform(newsgroups_df[\"text\"])\n",
    "cvectorsn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e15ed",
   "metadata": {},
   "source": [
    "Um Wörter weniger stark zu gewichten, die ein geringes Differenzierungspotenzial haben, kannst du das sog. [TF/IDF-Maß](https://de.wikipedia.org/wiki/Tf-idf-Ma%C3%9F) verwenden. `scikit-learn` kann diese Transformation direkt für dich durchführen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "tvectors = tfidf.fit_transform(cvectors)\n",
    "tvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb4e60",
   "metadata": {},
   "source": [
    "Die Dimensionen ändern sich dadurch nicht, aber für viele Machine Learning-Verfahren hast du damit eine bessere Darstellung der Dokumentvektoren gefunden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb875bac",
   "metadata": {},
   "source": [
    "## Sprache vektorisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024e4e9",
   "metadata": {},
   "source": [
    "Sprache lässt sich normalerweise nicht direkt vektorisieren. In den meisten Fällen wirst du ein sog. [Speech-to-Text-Verfahren](https://de.wikipedia.org/wiki/Spracherkennung) verwenden, um die gesprochene Sprache in Text zu wandeln. Anschließend kannst du die oben erklärten Methoden zur Textanalyse verwenden.\n",
    "\n",
    "Leider gehen bei dieser Methode Informationen verloren, die du nicht wieder rekonstruieren kannst. So bleiben etwa die *Emotionen* auf der Strecke, die durch Betonung etc. vermittelt werden. Wenn du solche Anforderungen hast, gibt es wieder spezielle Algorithmen, die dies extrahieren können. Dieses Gebiet ist allerdings noch ziemlich neu, so dass wir uns im Rahmen des Kurses damit nicht beschäftigen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd93d0e",
   "metadata": {},
   "source": [
    "## Bilder vektorisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365aaa15",
   "metadata": {},
   "source": [
    "Bilder sind deutlich schwieriger zu verarbeiten und zu vektorisieren. Zunächst werden die Bilder auf eine Einheitsgröße von z.B. 100x100 Pixel und Graustufen reduziert. Die daraus entstehenden 10.000 Werte repräsentieren dann ein Bild.\n",
    "\n",
    "Anders als bei Texten ist der unmittelbare Zusammenhang der Vektoren zu den Bildern (bzw. zu dem, was auf den Bilder dargestellt wird) nicht mehr gegeben. Daher wird für Machine Learning mit Bildern fast immer Deep Learning benutzt, damit im ersten Layer des Netzwerks durch das Training im Prinzip bessere Vektoren für die Bilder konstruiert werden können.\n",
    "\n",
    "Eine häufige Fragestellung bei Bilder ist die sog. *Object Detection*, d.h. du möchtest gerne wissen, was sich auf den Bilder befindet. Das Training dafür benötigt sehr große Datenmengen, d.h. sehr viele Bilder, bei denen die abgebildeten Objekte schon bekannt sind. Häufig gibt es dazu bereits vortrainierte Modelle, die dur direkt nutzen kannst. Diese übernehmen dann auch die Konvertierung der Bilder für dich.\n",
    "\n",
    "Beispiele für *Object Detection* findest du bei [YoloV5](https://github.com/ultralytics/yolov5), das ein sehr schnelles Modell bereitstellt. Möchtest du auch Konturen erkennen, kannst du z.B. [Detectron2](https://github.com/facebookresearch/detectron2) von Facebook Research verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82149ee5",
   "metadata": {},
   "source": [
    "## Videos vektorisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc16f6",
   "metadata": {},
   "source": [
    "Für Videos gelten ähnlich Bedingungen wie für Bilder - auch hier kannst du Objekte erkennen und diese dann verwenden.\n",
    "\n",
    "Erschwerend kommt hier allerdings dazu, dass die Objekte sich im Laufe der Zeit ändern, also eine weitere Dimension. Das kann helfen, aber auch zur deutliche Komplizierung des Vorgehens führen.\n",
    "\n",
    "Die meisten Videos sind außerdem mit Sprache versehen, die du mit den oben geschilderten Verfahren zunächst in Text wandeln solltest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e55ee",
   "metadata": {},
   "source": [
    "## Vektorisierung nicht unterschätzen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336346e",
   "metadata": {},
   "source": [
    "Ob du ein Machine Learning-Projekt erfolgreich abschließen kannst, liegt zu einem erheblichen Teil an der korrekten Vektorisierung. Dabei geht es vor allem auch darum, die richtigen *Features* zu finden.\n",
    "\n",
    "* Bei strukturierten Daten hast du es dabei am einfachsten - du kannst direkt mit Zahlen arbeiten, die du evtl. noch skalieren musst.\n",
    "* Bei Texten ist es etwas schwieriger, häufig kannst du mit der Document-Term-Matrix arbeiten und Wörter (oder Bigramme) als Features verwenden.\n",
    "* Sprache wandelst du am besten in Text.\n",
    "* Bei Bildern kannst du häufig die dort abgebildeten Objekte als Features verwenden.\n",
    "* Videos kombinieren Bilder mit Zeitabhängigkeit sowie Sprache und stellen daher eine echte Herausforderung dar.\n",
    "\n",
    "Dieses sog. *Feature Engineering* wird dir in vielen Projekten begegnen und ist in seiner Wichtigkeit kaum zu unterschätzen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
